import cv2
import os
import numpy as np
import threading
import torch
import datetime
from torchvision import transforms
from .detector import Detector
from .db import DatabaseManager
from .logging import logger

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ OpenCV
cv2.setNumThreads(0)
cv2.ocl.setUseOpenCL(False)


class VideoProcessor:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: YOLO-–¥–µ—Ç–µ–∫—Ü–∏—è, PyTorch-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ.
    """

    def __init__(
            self,
            model_path: str,
            device: str,
            video_path: str,
            tracker_config: str,
            pt_model_path: str,
            db_path: str
    ):
        self.model_path = model_path
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.video_path = video_path
        self.tracker_config = tracker_config
        self.pt_model_path = pt_model_path
        self.db_path = db_path
        self.is_image = video_path.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))

        logger.info(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã VideoProcessor: model={model_path}, device={self.device}, "
                    f"video={video_path}, tracker={tracker_config}, pt_model={pt_model_path}, db={db_path}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞
        if not os.path.exists(video_path):
            logger.error(f"üö® –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")
            raise ValueError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        try:
            self.detector = Detector(model_path, device, tracker_config)
            logger.info("‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {str(e)}")
            raise

        # –ó–∞–≥—Ä—É–∑–∫–∞ PyTorch –º–æ–¥–µ–ª–∏
        try:
            from torchvision import models
            self.db = DatabaseManager(db_path=db_path, yolo_model=self.detector)
            num_classes = len(self.db.load_class_names())
            self.model = models.mobilenet_v3_small(num_classes=num_classes)
            self.model.load_state_dict(torch.load(pt_model_path, map_location=self.device))
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"‚úÖ PyTorch –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {pt_model_path}")
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PyTorch –º–æ–¥–µ–ª–∏: {str(e)}")
            raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PyTorch –º–æ–¥–µ–ª–∏: {str(e)}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # –ê–Ω–∞–ª–æ–≥ TF [-1, 1]
        ])

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –ë–î
        self.class_names = self.db.load_class_names()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if self.is_image:
            self.image = cv2.imread(video_path)
            if self.image is None:
                logger.error(f"üö® –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {video_path}")
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {video_path}")
            logger.info(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–æ: {video_path}")
            self.cap = None
        else:
            self.cap = cv2.VideoCapture(video_path)
            if not self.cap.isOpened():
                logger.error(f"üö® –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")
            fps = self.cap.get(cv2.CAP_PROP_FPS)
            logger.info(f"üìπ –í–∏–¥–µ–æ –æ—Ç–∫—Ä—ã—Ç–æ: {video_path}, FPS: {fps}")
            self.image = None

        self._cap_lock = threading.Lock()
        self.logs = []
        self.track_labels = {}
        self.state_durations = {}
        self.running = True
        self.delay = 1 / 30  # FPS=30

    def reset_video(self):
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞."""
        if self.is_image:
            logger.info("üîÑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
            return True
        with self._cap_lock:
            self.cap.release()
            self.cap = cv2.VideoCapture(self.video_path)
            if not self.cap.isOpened():
                logger.error(f"üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –≤–∏–¥–µ–æ: {self.video_path}")
                return False
            logger.info(f"‚úÖ –í–∏–¥–µ–æ –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–æ: {self.video_path}")
            return True

    def preprocess_crop(self, roi):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–ª–∞—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞ –¥–ª—è PyTorch."""
        if roi.size == 0:
            return torch.zeros((3, 224, 224), device=self.device)
        img = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (224, 224))
        img = self.transform(img).to(self.device)
        return img

    def classify_pig_states(self, frame: np.ndarray, bboxes):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–≤–∏–Ω–µ–π —Å PyTorch."""
        try:
            crops = []
            valid_bboxes = []
            for bbox in bboxes:
                x1, y1, x2, y2 = map(int, bbox)
                if (x2 - x1) < 5 or (y2 - y1) < 5:
                    logger.warning(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –±–æ–∫—Å: {bbox.tolist()}")
                    continue
                crop = frame[y1:y2, x1:x2]
                if crop.size == 0:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π crop –¥–ª—è –±–æ–∫—Å–∞: {bbox.tolist()}")
                    continue
                crops.append(self.preprocess_crop(crop))
                valid_bboxes.append(bbox)

            if not crops:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö crops –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                return [None] * len(bboxes)

            # Batch-processing
            crops = torch.stack(crops)
            with torch.no_grad():
                preds = self.model(crops)
            state_indices = torch.argmax(preds, dim=1).cpu().numpy()
            return [state_indices[i] if i < len(state_indices) else None for i in range(len(bboxes))]

        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}")
            return [None] * len(bboxes)

    def process_frame(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π."""
        import time
        start_time = time.time()

        if self.is_image:
            frame = self.image
            if frame is None:
                logger.error("üö® –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
                return None
        else:
            with self._cap_lock:
                ret, frame = self.cap.read()
            if not ret or frame is None:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä")
                if not self.reset_video():
                    return None
                with self._cap_lock:
                    ret, frame = self.cap.read()
                if not ret or frame is None:
                    logger.error("üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
                    return None

        try:
            results = self.detector.detect_and_track(frame)
            if not results or not len(results) or results[0] is None:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏")
                return frame

            frame_drawn = results[0].plot(boxes=True, masks=True, labels=False)
            bboxes = [box.xyxy[0].cpu().numpy() for box in results[0].boxes if box.id is not None]
            track_ids = [int(box.id) for box in results[0].boxes if box.id is not None]

            if bboxes:
                state_indices = self.classify_pig_states(frame, bboxes)
                for track_id, state_idx, bbox in zip(track_ids, state_indices, bboxes):
                    if state_idx is None or not self.class_names:
                        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è track_id {track_id}")
                        continue
                    state = self.class_names[state_idx]
                    state_desc = self.db.get_states().get(state, "Unknown")
                    if state_desc == "Unknown":
                        logger.warning(f"‚ö†Ô∏è –°–æ—Å—Ç–æ—è–Ω–∏–µ {state} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ")
                        continue
                    self.track_labels.setdefault(track_id, {}).update(
                        {state: self.track_labels.get(track_id, {}).get(state, 0) + 1}
                    )
                    fps = self.cap.get(cv2.CAP_PROP_FPS) or 30 if not self.is_image else 30
                    self.state_durations.setdefault(track_id, {}).update(
                        {state: self.state_durations.get(track_id, {}).get(state, 0) + 1 / fps}
                    )
                    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    self.logs.append([track_id, f"{timestamp} - –°–≤–∏–Ω—å—è {track_id} –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ {state_desc}"])
                    logger.info(f"üè∑Ô∏è –û–±—ä–µ–∫—Ç: track_id={track_id}, state={state_desc}, bbox={bbox.tolist()}")

            logger.info(f"üïí –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫")
            return frame_drawn

        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞: {str(e)}")
            return frame

    def next_frame(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è SocketIO."""
        return self.process_frame()

    def release(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        if not self.is_image:
            with self._cap_lock:
                self.cap.release()
        self.running = False
        self.db.close()
        logger.info("üõë VideoProcessor –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    def __del__(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞."""
        if not self.is_image and hasattr(self, 'cap') and self.cap.isOpened():
            self.cap.release()
        if hasattr(self, 'db'):
            self.db.close()
        logger.info("üßπ –†–µ—Å—É—Ä—Å—ã VideoProcessor –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")