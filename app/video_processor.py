"""
–ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–≤–∏–Ω–µ–π.
–°–æ–¥–µ—Ä–∂–∏—Ç –∫–ª–∞—Å—Å—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –æ–±—ä–µ–∫—Ç–æ–≤.
"""

import os
import time
import datetime
import threading
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field

import cv2
import numpy as np
import torch
from torchvision import transforms
from PIL import Image, ImageDraw, ImageFont

from .detector import Detector
from .db import DatabaseManager
from .logging import logger

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è OpenCV
cv2.setNumThreads(0)
cv2.ocl.setUseOpenCL(False)


@dataclass
class ProcessingStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    frames_processed: int = 0
    detections_count: int = 0
    classifications_count: int = 0
    errors_count: int = 0
    start_time: float = field(default_factory=time.time)
    
    @property
    def uptime(self) -> float:
        """–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        return time.time() - self.start_time
    
    @property
    def fps(self) -> float:
        """–°—Ä–µ–¥–Ω–∏–π FPS –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        return self.frames_processed / max(self.uptime, 1)


class VideoProcessor:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: YOLO-–¥–µ—Ç–µ–∫—Ü–∏—è, PyTorch-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ.
    """

    def __init__(
            self,
            model_path: str,
            device: str,
            video_path: str,
            tracker_config: str,
            pt_model_path: str,
            db_path: str,
            conf: float = 0.5
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –≤–∏–¥–µ–æ.
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ YOLO
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ('cpu' –∏–ª–∏ 'cuda')
            video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–ª–∏ 'webcam'
            tracker_config: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç—Ä–µ–∫–µ—Ä–∞
            pt_model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ PyTorch –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            db_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            conf: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
        """
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.model_path = model_path
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.video_path = video_path
        self.tracker_config = tracker_config
        self.pt_model_path = pt_model_path
        self.db_path = db_path
        self.conf = float(conf)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        self.is_image = video_path.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))
        self.is_webcam = video_path.lower() in ['webcam', '0', '1', '2', '3']
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.stats = ProcessingStats()
        self.running = True
        self._cap_lock = threading.RLock()
        
        # –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–∫–∏–Ω–≥–∞
        self.logs: List[Tuple[int, str]] = []
        self.track_labels: Dict[int, Dict[str, int]] = {}
        self.state_durations: Dict[int, Dict[str, float]] = {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.conf_threshold = max(0.3, conf)
        self.delay = 1 / 30  # FPS=30
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VideoProcessor:")
        logger.info(f"   üìÅ –ú–æ–¥–µ–ª—å YOLO: {model_path}")
        logger.info(f"   üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        logger.info(f"   üìπ –ò—Å—Ç–æ—á–Ω–∏–∫: {video_path}")
        logger.info(f"   üéØ –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {self.conf}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._init_video_source()
        self._init_detector()
        self._init_database()
        self._init_pytorch_model()
        
        logger.info("‚úÖ VideoProcessor —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _init_video_source(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤–∏–¥–µ–æ."""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–ª—è –≤–µ–±-–∫–∞–º–µ—Ä—ã)
        if not self.is_webcam and not os.path.exists(self.video_path):
            raise ValueError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.video_path}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if self.is_webcam:
            self._init_webcam()
        elif self.is_image:
            self._init_image()
        else:
            self._init_video_file()
    
    def _init_webcam(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–±-–∫–∞–º–µ—Ä—ã."""
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–µ–±-–∫–∞–º–µ—Ä—É")
        
        fps = self.cap.get(cv2.CAP_PROP_FPS) or 30
        logger.info(f"üì∑ –í–µ–±-–∫–∞–º–µ—Ä–∞ –æ—Ç–∫—Ä—ã—Ç–∞, FPS: {fps}")
        self.image = None
    
    def _init_image(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        self.image = cv2.imread(self.video_path)
        if self.image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {self.video_path}")
        
        logger.info(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–æ: {self.video_path}")
        self.cap = None
    
    def _init_video_file(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞."""
        self.cap = cv2.VideoCapture(self.video_path)
        if not self.cap.isOpened():
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {self.video_path}")
        
        fps = self.cap.get(cv2.CAP_PROP_FPS)
        logger.info(f"üìπ –í–∏–¥–µ–æ –æ—Ç–∫—Ä—ã—Ç–æ: {self.video_path}, FPS: {fps}")
        self.image = None
    
    def _init_detector(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞."""
        try:
            self.detector = Detector(self.model_path, str(self.device), self.tracker_config)
            logger.info("‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä YOLO –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {str(e)}")
            raise
    
    def _init_database(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º singleton –∏–∑ StateManager –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–π –ë–î
            from .routes import StateManager
            self.db = StateManager.get_db()
            self.state_translations = self.db.get_translations('ru')
            self.class_names = self.db.load_class_names()
            logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∞, –ø–µ—Ä–µ–≤–æ–¥–æ–≤: {len(self.state_translations)}")
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {str(e)}")
            raise
    
    def _init_pytorch_model(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ PyTorch –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
        try:
            from torchvision import models
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à—Ä–∏—Ñ—Ç–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            self._init_font()
            
            num_classes = len(self.class_names)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            if not os.path.exists(self.pt_model_path):
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å PyTorch –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.pt_model_path}")
                self.model = None
            else:
                self._load_pytorch_model(num_classes)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ PyTorch –º–æ–¥–µ–ª–∏: {str(e)}")
            self.model = None
            self.transform = None
    
    def _init_font(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —à—Ä–∏—Ñ—Ç–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        font_paths = [
            "arial.ttf",
            "C:/Windows/Fonts/arial.ttf",
            "C:/Windows/Fonts/calibri.ttf"
        ]
        
        for font_path in font_paths:
            try:
                self.font = ImageFont.truetype(font_path, 20)
                logger.debug(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —à—Ä–∏—Ñ—Ç: {font_path}")
                return
            except Exception:
                continue
        
        # Fallback –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —à—Ä–∏—Ñ—Ç
        self.font = ImageFont.load_default()
        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —à—Ä–∏—Ñ—Ç")
    
    def _load_pytorch_model(self, num_classes: int) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ PyTorch."""
        from torchvision import models
        
        try:
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
            self.model = models.mobilenet_v3_small(weights=None)
            
            # –ó–∞–º–µ–Ω—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (–∫–∞–∫ –≤ train_classify.py)
            self.model.classifier[3] = torch.nn.Sequential(
                torch.nn.Linear(self.model.classifier[3].in_features, 256),
                torch.nn.ReLU(),
                torch.nn.Dropout(p=0.4),
                torch.nn.Linear(256, num_classes)
            )
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞
            try:
                state_dict = torch.load(self.pt_model_path, map_location=self.device)
                self.model.load_state_dict(state_dict)
                logger.info(f"‚úÖ PyTorch –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å {num_classes} –∫–ª–∞—Å—Å–∞–º–∏")
            except Exception as load_error:
                logger.error(f"üö® –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤: {str(load_error)}")
                logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å —Å–ª—É—á–∞–π–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏")
            
            self.model.to(self.device)
            self.model.eval()
            
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}")
            self.model = None

    def reset_video(self):
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞."""
        if self.is_image:
            logger.info("üîÑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
            return True
        with self._cap_lock:
            self.cap.release()
            if self.is_webcam:
                self.cap = cv2.VideoCapture(0)  # –í–µ–±-–∫–∞–º–µ—Ä–∞
                if not self.cap.isOpened():
                    logger.error("üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –≤–µ–±-–∫–∞–º–µ—Ä—É")
                    return False
                logger.info("‚úÖ –í–µ–±-–∫–∞–º–µ—Ä–∞ –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–∞")
            else:
                self.cap = cv2.VideoCapture(self.video_path)
                if not self.cap.isOpened():
                    logger.error(f"üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –≤–∏–¥–µ–æ: {self.video_path}")
                    return False
                logger.info(f"‚úÖ –í–∏–¥–µ–æ –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–æ: {self.video_path}")
            return True

    def filter_pig_detections(self, boxes, confidences):
        """
        –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—á—Ç–∏ –≤—Å–µ –¥–µ—Ç–µ–∫—Ü–∏–∏.
        
        Args:
            boxes: –°–ø–∏—Å–æ–∫ –±–æ–∫—Å–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ [x1, y1, x2, y2]
            confidences: –°–ø–∏—Å–æ–∫ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        """
        valid_indices = []
        
        for i, (box, conf) in enumerate(zip(boxes, confidences)):
            # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if conf < self.conf_threshold:
                logger.debug(f"üö´ –ë–æ–∫—Å {i} –æ—Ç–∫–ª–æ–Ω–µ–Ω: –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {conf:.2f} < {self.conf_threshold}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –±–æ–∫—Å—ã (–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã)
            x1, y1, x2, y2 = box
            width = x2 - x1
            height = y2 - y1
            area = width * height
            
            if area < 100:  # –¢–æ–ª—å–∫–æ —Å–∞–º—ã–µ –º–µ–ª–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
                logger.debug(f"üö´ –ë–æ–∫—Å {i} –æ—Ç–∫–ª–æ–Ω–µ–Ω: —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π {area}")
                continue
            
            valid_indices.append(i)
            logger.debug(f"‚úÖ –ë–æ–∫—Å {i} –ø—Ä–∏–Ω—è—Ç: area={area}, conf={conf:.2f}")
        
        logger.info(f"üéØ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(valid_indices)}/{len(boxes)} –±–æ–∫—Å–æ–≤ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É")
        return valid_indices

    def draw_russian_text(self, img, text, position, color=(0, 255, 0)):
        """
        –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é PIL.
        
        Args:
            img: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ OpenCV (BGR)
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
            position: –ü–æ–∑–∏—Ü–∏—è (x, y)
            color: –¶–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGB
        
        Returns:
            –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –Ω–∞–Ω–µ—Å–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        """
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB –¥–ª—è PIL
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(img_rgb)
            draw = ImageDraw.Draw(pil_img)
            
            # –†–∏—Å—É–µ–º —Ç–µ–∫—Å—Ç
            x, y = position
            draw.text((x, y), text, font=self.font, fill=color)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ BGR –¥–ª—è OpenCV
            img_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
            return img_bgr
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenCV
            cv2.putText(img, text, position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            return img

    def preprocess_crop(self, roi):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–ª–∞—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞ –¥–ª—è PyTorch."""
        if roi.size == 0:
            return torch.zeros((3, 224, 224), device=self.device)
        img = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (224, 224))
        img = self.transform(img).to(self.device)
        return img

    def classify_pig_states(self, frame: np.ndarray, bboxes):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–≤–∏–Ω–µ–π —Å PyTorch. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã, —á—Ç–æ –∏ bboxes."""
        try:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            if self.model is None:
                logger.warning("‚ö†Ô∏è PyTorch –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
                import random
                return [random.randint(0, len(self.class_names)-1) if len(self.class_names) > 0 else 0 for _ in bboxes]
            
            crops = []
            orig_indices = []
            for idx, bbox in enumerate(bboxes):
                x1, y1, x2, y2 = map(int, bbox)
                if (x2 - x1) < 5 or (y2 - y1) < 5:
                    logger.warning(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –±–æ–∫—Å: {bbox.tolist()}")
                    continue
                crop = frame[y1:y2, x1:x2]
                if crop.size == 0:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π crop –¥–ª—è –±–æ–∫—Å–∞: {bbox.tolist()}")
                    continue
                crops.append(self.preprocess_crop(crop))
                orig_indices.append(idx)

            states_aligned = [None] * len(bboxes)
            if not crops:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö crops –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                return states_aligned

            # Batch-processing
            crops = torch.stack(crops)
            with torch.no_grad():
                preds = self.model(crops)
            state_indices = torch.argmax(preds, dim=1).cpu().numpy().tolist()
            for i, idx in enumerate(orig_indices):
                if i < len(state_indices):
                    states_aligned[idx] = state_indices[i]
            return states_aligned

        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}")
            return [None] * len(bboxes)

    def process_frame(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π."""
        import time
        start_time = time.time()

        if self.is_image:
            frame = self.image
            if frame is None:
                logger.error("üö® –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
                return None
        else:
            with self._cap_lock:
                ret, frame = self.cap.read()
            if not ret or frame is None:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä")
                if not self.reset_video():
                    return None
                with self._cap_lock:
                    ret, frame = self.cap.read()
                if not ret or frame is None:
                    logger.error("üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞")
                    return None

        try:
            results = self.detector.detect_and_track(frame, conf=self.conf)
            if not results or not len(results) or results[0] is None:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏")
                return frame

            result = results[0]
            if not hasattr(result, 'boxes') or result.boxes is None or len(result.boxes) == 0:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –±–æ–∫—Å–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –¥–µ—Ç–µ–∫—Ü–∏–∏")
                return frame

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ—Ç–µ–∫—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (–Ω–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–∞—è)
            if hasattr(result, 'masks') and result.masks is not None:
                logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –Ω–æ –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–µ—Ç–µ–∫—Ü–∏–æ–Ω–Ω–∞—è")
                frame_drawn = result.plot(boxes=True, masks=True, labels=False)
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                frame_drawn = result.plot(boxes=True, labels=False)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –±–æ–∫—Å—ã –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            boxes_iter = list(result.boxes)
            all_bboxes = [box.xyxy[0].cpu().numpy() for box in boxes_iter]
            all_confidences = [box.conf.item() for box in boxes_iter]
            all_track_ids = [int(box.id.item()) if getattr(box, 'id', None) is not None else idx for idx, box in enumerate(boxes_iter)]

            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_bboxes)} –æ–±—ä–µ–∫—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            valid_indices = self.filter_pig_detections(all_bboxes, all_confidences)
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
            bboxes = [all_bboxes[i] for i in valid_indices]
            track_ids = [all_track_ids[i] for i in valid_indices]

            logger.info(f"üéØ –û—Å—Ç–∞–ª–æ—Å—å {len(bboxes)} –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")

            if bboxes and len(self.class_names) > 0:
                state_indices = self.classify_pig_states(frame, bboxes)
                successful_classifications = 0
                
                for track_id, state_idx, bbox in zip(track_ids, state_indices, bboxes):
                    if state_idx is None:
                        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è track_id {track_id}")
                        continue
                    
                    if state_idx >= len(self.class_names):
                        logger.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_idx} –¥–ª—è track_id {track_id}")
                        continue
                        
                    state = self.class_names[state_idx]
                    state_desc = self.db.get_states().get(state, state)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º –∫–æ–¥ –µ—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç
                    state_ru = self.state_translations.get(state, state)  # –ü–æ–ª—É—á–∞–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
                    
                    self.track_labels.setdefault(track_id, {}).update(
                        {state: self.track_labels.get(track_id, {}).get(state, 0) + 1}
                    )
                    fps = self.cap.get(cv2.CAP_PROP_FPS) or 30 if not self.is_image else 30
                    self.state_durations.setdefault(track_id, {}).update(
                        {state: self.state_durations.get(track_id, {}).get(state, 0) + 1 / fps}
                    )
                    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    self.logs.append([track_id, f"{timestamp} - –°–≤–∏–Ω—å—è {track_id} –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ {state_ru}"])
                    logger.info(f"üè∑Ô∏è –û–±—ä–µ–∫—Ç: track_id={track_id}, state={state_ru} ({state}), bbox={bbox.tolist()}")
                    
                    # –†–∏—Å—É–µ–º –ø–æ–¥–ø–∏—Å–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –∫–∞–¥—Ä–µ (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ)
                    x1, y1, x2, y2 = map(int, bbox)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PIL –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    try:
                        frame_drawn = self.draw_russian_text(
                            frame_drawn, 
                            f"{track_id}:{state_ru}",
                            (x1, max(0, y1 - 25)),
                            color=(0, 255, 0)  # RGB —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PIL
                        )
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º ASCII")
                        # Fallback –Ω–∞ ASCII –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –æ—Ç—Ä–∏—Å–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                        cv2.putText(
                            frame_drawn,
                            f"{track_id}:{state}",
                            (x1, max(0, y1 - 10)),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.6,
                            (0, 255, 0),
                            2,
                            cv2.LINE_AA
                        )
                    successful_classifications += 1
                
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ {successful_classifications} –∏–∑ {len(bboxes)} –æ–±—ä–µ–∫—Ç–æ–≤")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –±–æ–∫—Å–æ–≤ –∏–ª–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")

            logger.info(f"üïí –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫")
            return frame_drawn

        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞: {str(e)}")
            import traceback
            logger.error(f"üö® –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
            return frame

    def next_frame(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è SocketIO."""
        return self.process_frame()

    def release(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        if not self.is_image:
            with self._cap_lock:
                self.cap.release()
        self.running = False
        # –ë–î –±–æ–ª—å—à–µ –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ singleton
        logger.info("üõë VideoProcessor –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    def __del__(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞."""
        if not self.is_image and hasattr(self, 'cap') and self.cap.isOpened():
            self.cap.release()
        # –ë–î –±–æ–ª—å—à–µ –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ singleton
        logger.info("üßπ –†–µ—Å—É—Ä—Å—ã VideoProcessor –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")