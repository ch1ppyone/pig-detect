# Документация системы мониторинга поведения свиней

Система построена на Python с использованием PyTorch для моделей ИИ, Flask для веб-сервера и SocketIO для реального времени. Она детектирует свиней на видео/изображениях с помощью YOLO, классифицирует их позы с помощью MobileNetV3Small и предоставляет веб-интерфейс для мониторинга, управления данными и обучения моделей.

## 1. Структура проекта

Проект организован модульно. Вот ключевые папки и файлы:

- **app/**: Содержит логику обработки видео, детекцию, классификацию, базу данных и маршруты Flask.
  - `logging.py`: Настройка логирования.
  - `video_processor.py`: Обработка кадров (детекция YOLO + классификация PyTorch).
  - `__init__.py`: Инициализация Flask приложения и SocketIO.
  - `config.py`: Конфигурация (пути, устройство, проверки hardware).
  - `detector.py`: Детектор на базе YOLO.
  - `db.py`: Менеджер SQLite базы данных для состояний свиней.
  - `routes.py`: Маршруты Flask и обработчики SocketIO (видео-поток, логи, обучение, управление данными).

- **train/**: Скрипты для обучения моделей.
  - `train_pt.py`: Конвертация датасета и обучение MobileNetV3Small (PyTorch) для классификации поз.
  - `train_yolo.py`: Обучение YOLO для детекции свиней.

- **run.py**: Точка входа для запуска Flask-сервера.

- **tests/**: Unit-тесты.
  - `test_db.py`: Тесты для базы данных.
  - `test_detector.py`: Тесты для YOLO-детектора.

- **Другие файлы/папки**:
  - `.env`: Конфигурация переменных окружения (MODEL_PATH, PT_MODEL_PATH, DB_PATH и т.д.).
  - `pig_states.db`: SQLite база данных (создается автоматически).
  - `models/`: Директория для моделей (yolo11m-seg.pt, pig_model.pth).
  - `train/dataset/`: Автоматически генерируемый датасет для обучения поз (после `train_pt.py --convert`).
  - `train/object/pig/`: Датасет для YOLO (images/ и labels/ с .txt аннотациями).
  - `train/models/`: Директория для обученных моделей YOLO (например, yolo11m-seg-mod.pt), которые нужно переносить в `models/` для использования в системе.

- **test_data/**: Тестовые видео/изображения (pigs.mp4, 2.mp4 и т.д.), используемые для обработки.

Временные папки (например, `temp/` от обучения YOLO) генерируются и удаляются автоматически.

## 2. Необходимые библиотеки для установки

Система требует Python 3.11+. Установите библиотеки через `pip`. Рекомендуется создать виртуальное окружение (`python -m venv env; source env/bin/activate` на Unix или `env\Scripts\activate` на Windows).

Список библиотек:
- flask
- flask-socketio
- ultralytics
- torch
- torchvision
- opencv-python
- numpy
- python-dotenv
- tqdm
- pyyaml

Установка:
```bash
pip install flask flask-socketio ultralytics torch torchvision opencv-python numpy python-dotenv tqdm pyyaml
```

Для GPU: Установите PyTorch с CUDA (см. раздел 7.1).

## 3. Описание файлов, методов и функционала

### app/logging.py
Настраивает глобальный логгер для вывода сообщений в консоль с временными метками и уровнями (INFO, WARNING, ERROR). Логирование используется во всех модулях для отслеживания событий, ошибок и прогресса.
- **Процесс**: Вызывает `logging.basicConfig` с форматом `%(asctime)s - %(levelname)s - %(message)s`. Создает логгер `logger = logging.getLogger(__name__)`, который импортируется в другие файлы для вызова `logger.info()`, `logger.warning()` и т.д.

### app/video_processor.py
Класс VideoProcessor обрабатывает видеопоток или изображение: загружает видео, выполняет детекцию свиней YOLO, классифицирует их позы PyTorch, логирует результаты и управляет потоками.
- **__init__(model_path, device, video_path, tracker_config, pt_model_path, db_path)**: 
  - Проверяет существование файлов (видео, модели).
  - Инициализирует YOLO-детектор (`Detector`).
  - Загружает PyTorch модель MobileNetV3Small, переводит в режим оценки (`eval()`) и на устройство (GPU/CPU).
  - Настраивает трансформации (`transforms.Compose`) для нормализации изображений в диапазон [-1, 1].
  - Загружает классы поз из базы данных.
  - Открывает видео (`cv2.VideoCapture`) или изображение (`cv2.imread`), устанавливает задержку кадров.
  - Инициализирует локеры, логи и словари для состояний.
- **reset_video()**: Закрывает и переоткрывает видео при ошибке чтения кадра, используя `cv2.VideoCapture`.
- **preprocess_crop(roi)**: Преобразует область интереса (ROI) в тензор: конвертирует BGR в RGB (`cv2.cvtColor`), ресайзит до 224x224 (`cv2.resize`), применяет трансформации (`ToTensor`, `Normalize`).
- **classify_pig_states(frame, bboxes)**: 
  - Фильтрует маленькие/пустые bbox.
  - Создает кропы из frame, преобразует в тензоры.
  - Стэкает тензоры в батч (`torch.stack`), выполняет инференс (`model(crops)`).
  - Возвращает индексы состояний (`torch.argmax(preds, dim=1)`).
- **process_frame()**: 
  - Читает кадр из видео/изображения.
  - Выполняет детекцию (`detector.detect_and_track`), рисует bbox/маски (`results[0].plot`).
  - Классифицирует позы, обновляет словари состояний и длительностей, добавляет записи в логи.
  - Логирует время обработки.
- **next_frame()**: Вызывает `process_frame` для SocketIO.
- **release()**: Освобождает ресурсы (закрывает видео, базу данных).
- **__del__()**: Деструктор для очистки при удалении объекта.

### app/__init__.py
Инициализирует Flask-приложение и SocketIO, создает глобальные переменные для очередей и задач.
- **create_app(processor=None)**: 
  - Проверяет hardware (`check_hardware`).
  - Создает VideoProcessor, если не передан.
  - Импортирует маршруты.

### app/config.py
Загружает конфигурацию из .env, парсит аргументы, проверяет устройство.
- **check_hardware()**: Логирует версии PyTorch, доступность CUDA и устройство.
- **create_video_processor(video_path=VIDEO_PATH)**: Создает экземпляр VideoProcessor с параметрами из .env.
- **get_color_for_track(track_id)**: Генерирует случайный цвет для трека по seed.
- **is_safe_filename(filename)**: Проверяет имя файла на безопасность (регулярное выражение).

### app/detector.py
Класс Detector для YOLO-детекции.
- **__init__(model_path, device, tracker_config)**: 
  - Инициализирует YOLO (`YOLO(model_path)`), переводит на устройство.
  - Трекер отключен (tracker_available=False).
- **detect_and_track(frame)**: Предсказывает на кадре (`detector.predict`), возвращает результаты, логирует количество объектов.

### app/db.py
Класс DatabaseManager для работы с SQLite базой состояний свиней.
- **__init__(db_path, yolo_model=None)**: Инициализирует базу, вызывает `init_db`.
- **init_db()**: Создает таблицу states, если нет, заполняет начальными состояниями (Feeding и т.д.).
- **get_states()**: Кэширует и возвращает словарь состояний (code: description).
- **load_class_names()**: Возвращает список кодов состояний.
- **add_state(code, desc)**: Добавляет состояние в базу и создает папку в `train/states/`.
- **update_state(old, new, desc)**: Обновляет состояние, переименовывает папку если нужно.
- **delete_state(code)**: Удаляет состояние из базы и папку с файлами.
- **close()**: Очищает кэш.

### app/routes.py
Определяет маршруты Flask и обработчики SocketIO.
- **frame_process_thread(client_id)**: Поток: обрабатывает кадры из процессора, кодирует в JPEG (`cv2.imencode`), добавляет в очередь.
- **frame_emit_thread()**: Поток: Отправляет кадры клиентам через SocketIO (`video_frame`).
- **data_emit_thread(client_id)**: Поток: Отправляет логи клиентам (`log_update`).
- **@app.route("/")**: Рендерит `index.html` с источниками видео.
- **@app.route("/get_states")**: Возвращает JSON состояний из базы.
- **@app.route("/get_videos/<state_code>")**: Возвращает пагинированный список файлов датасета для состояния.
- **@app.route("/train/dataset/<state_code>/<filename>")**: Отправляет файл датасета.
- **@app.route("/upload_video/<state_code>", methods=["POST"])**: Загружает файл в датасет, проверяет формат и безопасность.
- **@app.route("/delete_video/<state_code>", methods=["POST"])**: Удаляет файл из датасета.
- **@app.route("/train_model", methods=["POST"])**: Запускает поток обучения (`train_pt.py`), отправляет логи через SocketIO.
- **@socketio.on('connect')**: Создает процессор для клиента, запускает потоки.
- **@socketio.on('disconnect')**: Освобождает ресурсы.
- **@socketio.on('set_video_source')**: Меняет источник видео для клиента.
- **@socketio.on('request_logs')**: Отправляет логи.
- **@socketio.on('request_chart_data')**: Отправляет данные для графиков.
- **@socketio.on('add_state')**: Добавляет состояние в базу.
- **@socketio.on('update_state')**: Обновляет состояние.
- **@socketio.on('delete_state')**: Удаляет состояние.

### train/train_pt.py
Скрипт для конвертации YOLO-датасета и обучения MobileNetV3Small на PyTorch.
- **convert_yolo_to_tf(img_dir, label_dir, out_dir, db)**: Конвертирует YOLO-лейблы в папки с кропами по классам поз, используя bbox для вырезания ROI.
- **train_model(db)**: 
  - Загружает датасет (`ImageFolder`, `random_split`).
  - Инициализирует модель (`mobilenet_v3_small`), заменяет классификатор.
  - Обучает в 2 этапа: верхушка, затем fine-tuning последних слоев, с прогресс-барами (`tqdm`), потерями и точностью.
  - Сохраняет модель (`torch.save`).

### train/train_yolo.py
Скрипт для обучения YOLO на детекции свиней.
- **check_gpu()**: Проверяет CUDA и возвращает устройство.
- **create_default_yaml(dataset_path, class_name)**: Создает `data.yaml` с путями и классами.
- **update_yaml_paths(data_yaml_path, base_path)**: Абсолютизирует пути в YAML.
- **validate_dataset(dataset_path)**: Проверяет структуру датасета.
- **train_model(dataset_path, output_dir, class_name, epochs, img_size, batch_size)**: 
  - Валидирует датасет, обновляет YAML.
  - Инициализирует YOLO, обучает (`model.train`), сохраняет лучшую модель (с суффиксом -mod.pt в `train/models/`).

### run.py
Запускает Flask-сервер.
- **parse_args()**: Парсит аргументы командной строки (device, model, video и т.д.).
- **main**: Проверяет hardware, создает процессор, запускает Flask на 0.0.0.0:9001.

## 4. Модели, используемые в системе

- **YOLO11m-seg (из Ultralytics)**: Основная модель для инстанс-сегментации свиней (детекция bbox + маски). Выбрана за скорость (реальное время на GPU, ~15ms/кадр), точность (mAP >80% на COCO) и простоту (готовый API). Суффикс `-seg` обеспечивает точное выделение контуров свиней, что полезно для анализа форм или исключения фона. Отличается от обычной YOLO11m (без -seg): обычная дает только bbox и классы, без пиксельных масок; сегментационная добавляет маски для каждого объекта, что увеличивает точность в задачах мониторинга поведения (например, различение поз по силуэту). Предобученная на COCO, дообучается на датасете свиней для класса "pig".

- **MobileNetV3Small (из torchvision/PyTorch)**: Для классификации поз свиней (Feeding, Lateral_Lying и т.д.). Выбрана за легковесность (~5M параметров, ~2-4MB модель), скорость инференса (~10ms/bbox) и точность на ImageNet (>70%). Подходит для сервера. Предобученная на ImageNet, дообучается на кропах свиней.

Модели хранятся в `models/`, пути в `.env`. После обучения YOLO модель сохраняется в `train/models/` (с суффиксом -mod.pt) и требует переноса в `models/` для использования.

## 5. Требования к датасету

- **Для YOLO (детекция свиней, `train/object/pig/`)**:
  - Структура: `train/images/` (изображения .jpg/.png), `train/labels/` (.txt файлы в формате YOLO: class_id x_center y_center width height, class_id=0 для "pig").
  - Аналогично для `val/` (валидация).
  - Требования: >500 изображений на split, разнообразие (разные углы, освещение), аннотации bbox (используйте LabelImg или Roboflow).
  - Размер: 640x640 рекомендуемый, но модель адаптируется.

- **Для MobileNetV3Small (классификация поз, генерируется через `train_pt.py --convert`)**:
  - Автоматическая конвертация из YOLO-датасета: Кропы по bbox сохраняются в `train/dataset/<state>/` (например, `Feeding/`, `Standing/`).
  - Требования: >100 кропов на класс (5 классов из `pig_states.db`), баланс классов, разнообразие поз. После конвертации — стандартный ImageFolder формат.

Датасет должен быть чистым (без артефактов), аннотированным.

## 6. Возможные проблемы

- **Ошибки детекции**: Нет объектов — видео не содержит свиней или модель не дообучена. Решение: Переобучить YOLO, проверить видео.
- **Ошибки классификации**: Позы не определяются — модель не обучена или кропы пустые. Решение: Проверить логи `classify_pig_states`, переобучить `train_pt.py`.
- **Производительность**: Медленная обработка — слабый GPU/CPU. Решение: Уменьшить batch_size в обучении, использовать AMP в YOLO.
- **Ошибки импорта**: `cv2 not defined` или подобные — забыли импорт. Решение: Добавить `import cv2` в файл.
- **404 для файлов**: Файл отсутствует. Решение: Создать файл в нужной папке.
- **База данных**: `pig_states.db` не создается — права доступа. Решение: Проверить директорию.
- **CUDA ошибки**: Нет GPU — fallback на CPU, но медленнее. Решение: Установить CUDA Toolkit и драйверы NVIDIA.

Логи помогают диагностике (ищите `🚨` для ошибок).

## 7. Инструкции по запуску

### 7.1 Инструкция по установке CUDA для GPU
1. Проверьте совместимость: NVIDIA GPU с Compute Capability >=3.5 (проверьте на developer.nvidia.com/cuda-gpus).
2. Установите драйверы NVIDIA: Скачайте с nvidia.com/drivers (версия >=535 для CUDA 12.x).
3. Установите CUDA Toolkit: Скачайте с developer.nvidia.com/cuda-downloads (версия 12.x для PyTorch 2.7.1).
   - Выполните установщик, добавьте в PATH: `export PATH=/usr/local/cuda-12.x/bin:$PATH` (Unix) или аналогично на Windows.
4. Установите cuDNN: Скачайте с developer.nvidia.com/cudnn (версия для CUDA 12.x), скопируйте файлы в `/usr/local/cuda/`.
5. Установите PyTorch с CUDA: `pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu128`.
6. Проверьте: `python -c "import torch; print(torch.cuda.is_available())"` — должно вывести True.

### 7.2 Вариант для CPU
1. Установите PyTorch без CUDA: `pip install torch torchvision`.
2. В `.env` укажите `device=cpu`.
3. Запустите без GPU-аргументов. Производительность ниже (обработка кадров ~0.5-1 сек), но система работоспособна.

### Основная инструкция
1. Клонируйте репозиторий: `git clone <repo_url>`.
2. Создайте виртуальное окружение: `python -m venv env; source env/bin/activate`.
3. Установите библиотеки: `pip install -r requirements.txt`.
4. Обучите модели:
   - Детекция: `python train/train_yolo.py --dataset_dir train/object --class_name pig`. Перенесите выходную модель (например, yolo11m-seg-mod.pt) из `train/models/` в `models/`, обновите MODEL_PATH в .env.
   - Классификация: `python train/train_pt.py --convert`.
5. Запустите сервер: `python run.py --device cuda --video test_data/pigs.mp4` (или cpu).
6. Откройте браузер: `http://127.0.0.1:9001`.
7. Тесты: `python -m unittest discover tests`.