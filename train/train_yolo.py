import os
import argparse
import yaml
import torch
import shutil
from ultralytics import YOLO
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –∫–æ–Ω—Å–æ–ª—å, —Ä—É—Å—Å–∫–∏–π)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)

def check_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU."""
    logger.info(f"üî• –í–µ—Ä—Å–∏—è PyTorch: {torch.__version__}")
    cuda_v = torch.version.cuda or "none"
    cudnn_v = torch.backends.cudnn.version() or "none"
    logger.info(f"üîß –°–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–æ —Å CUDA: {cuda_v}")
    logger.info(f"üîß –í–µ—Ä—Å–∏—è cuDNN: {cudnn_v}")

    if torch.cuda.is_available():
        count = torch.cuda.device_count()
        logger.info(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {count} CUDA-—É—Å—Ç—Ä–æ–π—Å—Ç–≤")
        for idx in range(count):
            name = torch.cuda.get_device_name(idx)
            props = torch.cuda.get_device_properties(idx)
            total_mem = props.total_memory / (1024 ** 3)
            logger.info(f"üéÆ GPU {idx}: {name}, {total_mem:.1f} GB")
        return 'cuda:0'
    else:
        logger.warning("‚ö†Ô∏è torch.cuda.is_available() == False ‚Üí GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
        logger.info("üìù –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
        logger.info("üìù –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –¥—Ä–∞–π–≤–µ—Ä–∞ NVIDIA –∏ CUDA Toolkit")
        return 'cpu'

def create_default_yaml(dataset_path, class_name):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ data.yaml."""
    default_yaml = {
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': 1,
        'names': [class_name]
    }
    yaml_path = os.path.join(dataset_path, 'data.yaml')
    with open(yaml_path, 'w') as f:
        yaml.safe_dump(default_yaml, f)
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª {yaml_path} —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:")
    logger.info(f"üìã train: train/images")
    logger.info(f"üìã val: val/images")
    logger.info(f"üìã test: test/images")
    logger.info(f"üìã nc: 1")
    logger.info(f"üìã names: [{class_name}]")
    return yaml_path

def update_yaml_paths(data_yaml_path, base_path):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –≤ data.yaml."""
    try:
        with open(data_yaml_path, 'r') as f:
            data = yaml.safe_load(f)

        if not data:
            raise ValueError(f"–§–∞–π–ª {data_yaml_path} –ø—É—Å—Ç –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")

        for split in ['train', 'val', 'test']:
            if split in data and data[split]:
                rel = data[split]
                abs_path = os.path.abspath(os.path.join(base_path, rel))
                if not os.path.exists(abs_path):
                    logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ¬´{abs_path}¬ª –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                data[split] = abs_path

        updated_yaml = os.path.join(base_path, 'updated_data.yaml')
        with open(updated_yaml, 'w') as f:
            yaml.safe_dump(data, f)

        logger.info(f"üìã –°–æ–¥–µ—Ä–∂–∏–º–æ–µ {updated_yaml}:")
        with open(updated_yaml, 'r') as f:
            logger.info(f.read())

        return updated_yaml

    except Exception as e:
        logger.error(f"üö® –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {data_yaml_path}: {e}")
        raise

def validate_dataset(dataset_path):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    data_yaml = os.path.join(dataset_path, 'data.yaml')
    if not os.path.exists(data_yaml):
        raise FileNotFoundError(f"üö® –ù–µ –Ω–∞–π–¥–µ–Ω {data_yaml}")

    with open(data_yaml, 'r') as f:
        data = yaml.safe_load(f)

    for split in ['train', 'val']:
        split_path = os.path.join(dataset_path, data[split])
        if not os.path.exists(split_path) or not os.listdir(split_path):
            raise ValueError(f"üö® –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {split_path} –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

def train_model(dataset_path, output_dir, class_name, epochs=50, img_size=640, batch_size=16):
    """–û–±—É—á–µ–Ω–∏–µ YOLO –º–æ–¥–µ–ª–∏."""
    try:
        validate_dataset(dataset_path)
        data_yaml = os.path.join(dataset_path, 'data.yaml')
        updated_data_yaml = update_yaml_paths(data_yaml, dataset_path)
        device = check_gpu()
        model = YOLO(os.getenv('MODEL_PATH', 'models/yolo11m-seg.pt'))

        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {dataset_path} (device={device})")
        results = model.train(
            data=updated_data_yaml,
            epochs=epochs,
            imgsz=img_size,
            batch=batch_size,
            name='temp',
            project='temp',
            task='detect',
            device=device,
            patience=10,
            save=True,
            exist_ok=True
        )

        best = os.path.join('temp', 'temp', 'weights', 'best.pt')
        if not os.path.exists(best):
            raise RuntimeError("üö® –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–æ best.pt –Ω–µ –Ω–∞–π–¥–µ–Ω")

        os.makedirs(output_dir, exist_ok=True)
        final = os.path.join(output_dir, "yolo11m-seg-mod.pt")
        os.replace(best, final)

        shutil.rmtree('temp', ignore_errors=True)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final}")
        return final

    except Exception as e:
        logger.error(f"üö® –û—à–∏–±–∫–∞ –≤ train_model: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ YOLO –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
    parser.add_argument('--dataset_dir', type=str, default='../train/object', help="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument('--output_dir', type=str, default=os.getenv('MODEL_DIR', 'models'), help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    parser.add_argument('--class_name', type=str, default='pig', help="–ò–º—è –∫–ª–∞—Å—Å–∞")
    parser.add_argument('--epochs', type=int, default=50, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument('--img_size', type=int, default=640, help="–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞")
    parser.add_argument('--batch_size', type=int, default=16, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    args = parser.parse_args()

    dataset_path = os.path.join(args.dataset_dir, args.class_name)
    if not os.path.exists(dataset_path):
        logger.error(f"üö® –ü–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dataset_path}")
        return

    logger.info(f"üè∑Ô∏è –î–∞—Ç–∞—Å–µ—Ç: {dataset_path} | –∫–ª–∞—Å—Å: {args.class_name}")
    train_model(
        dataset_path=dataset_path,
        output_dir=args.output_dir,
        class_name=args.class_name,
        epochs=args.epochs,
        img_size=args.img_size,
        batch_size=args.batch_size
    )

if __name__ == '__main__':
    main()