#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
üèãÔ∏è‚Äç‚ôÇÔ∏è –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è YOLO-–¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ PyTorch –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑ —Å–≤–∏–Ω–µ–π.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å PyTorch (pig_model.pth) –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
"""

import os
import sys
import argparse
import cv2
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms, datasets
from torch.utils.data import DataLoader
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from app.db import DatabaseManager
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –∫–æ–Ω—Å–æ–ª—å, —Ä—É—Å—Å–∫–∏–π)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 20
VAL_SPLIT = 0.20
SEED = 42
DATASET_DIR = "../train/dataset/"
MODEL_DIR = os.getenv('MODEL_DIR', '../models')
MODEL_OUT = os.getenv('PT_MODEL_PATH', os.path.join(MODEL_DIR, "pig_model.pth"))
DB_PATH = os.getenv('DB_PATH', 'pig_states.db')
IMG_DIR = os.getenv('IMG_DIR', 'train/object/pig_pose/train/images')
LABEL_DIR = os.getenv('LABEL_DIR', 'train/object/pig_pose/train/labels')


def convert_yolo_to_tf(img_dir=IMG_DIR, label_dir=LABEL_DIR, out_dir=DATASET_DIR, db=None):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è YOLO-–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PyTorch."""
    logger.info("üîß –ù–∞—á–∞–ª–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ YOLO-–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PyTorch")
    class_names = db.load_class_names()

    if os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    os.makedirs(out_dir)

    for cls in class_names:
        cls_dir = os.path.join(out_dir, cls)
        os.makedirs(cls_dir, exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∞: {cls_dir}")

    processed = 0
    skipped = 0

    for filename in os.listdir(img_dir):
        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')):
            continue

        name = os.path.splitext(filename)[0]
        img_path = os.path.join(img_dir, filename)
        label_path = os.path.join(label_dir, f"{name}.txt")

        if not os.path.exists(label_path):
            logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –º–µ—Ç–∫–∏: {label_path}")
            skipped += 1
            continue

        img = cv2.imread(img_path)
        if img is None:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img_path}")
            skipped += 1
            continue

        h, w = img.shape[:2]

        with open(label_path, "r") as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]

        if not lines:
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π .txt —Ñ–∞–π–ª: {label_path}")
            skipped += 1
            continue

        try:
            cls_id, x, y, bw, bh = map(float, lines[0].split())
            cls_id = int(cls_id)
            if cls_id >= len(class_names):
                logger.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π ID –∫–ª–∞—Å—Å–∞ {cls_id} –≤ {label_path}")
                skipped += 1
                continue
            cls_name = class_names[cls_id]
        except Exception as e:
            logger.error(f"üö® –û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ —Å—Ç—Ä–æ–∫–∏ –≤ {label_path}: {e}")
            skipped += 1
            continue

        cx, cy, bw, bh = x * w, y * h, bw * w, bh * h
        x1 = int(cx - bw / 2)
        y1 = int(cy - bh / 2)
        x2 = int(cx + bw / 2)
        y2 = int(cy + bh / 2)

        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)

        roi = img[y1:y2, x1:x2]
        if roi.size == 0 or (x2 - x1) < 5 or (y2 - y1) < 5:
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π/—Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π bbox –≤ {label_path}")
            skipped += 1
            continue

        out_path = os.path.join(out_dir, cls_name, f"{name}.jpg")
        cv2.imwrite(out_path, roi)
        logger.info(f"‚úÖ {filename} ‚Üí {cls_name} (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {out_path})")
        processed += 1

    logger.info(f"üéâ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")


def train_model(db):
    """–û–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ PyTorch –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    try:
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        train_transform = transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.RandomAffine(degrees=0, scale=(0.9, 1.1)),
            transforms.RandomGrayscale(p=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        val_transform = transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
        full_dataset = datasets.ImageFolder(DATASET_DIR)
        train_size = int((1 - VAL_SPLIT) * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED)
        )
        train_dataset.dataset.transform = train_transform
        val_dataset.dataset.transform = val_transform

        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    except Exception as e:
        logger.error(f"üö® –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        raise

    class_names = db.load_class_names()
    num_classes = len(class_names)
    logger.info(f"üè∑Ô∏è –ö–ª–∞—Å—Å—ã: {class_names}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    try:
        model = models.mobilenet_v3_small(weights='DEFAULT')
        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)
        model = model.to(device)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å MobileNetV3Small –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        logger.error(f"üö® –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    # –≠—Ç–∞–ø 1: –û–±—É—á–µ–Ω–∏–µ –≤–µ—Ä—Ö—É—à–∫–∏
    logger.info("üöÄ –≠—Ç–∞–ø 1: –æ–±—É—á–µ–Ω–∏–µ –≤–µ—Ä—Ö—É—à–∫–∏...")
    model.classifier.train()
    for param in model.features.parameters():
        param.requires_grad = False

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        running_correct = 0
        total = 0
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{EPOCHS}", unit="batch")
        for inputs, labels in progress_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            running_correct += (predicted == labels).sum().item()
            total += labels.size(0)
            acc = 100 * running_correct / total
            progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = 100 * running_correct / total
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch+1}/{EPOCHS}, –ü–æ—Ç–µ—Ä–∏: {epoch_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {epoch_acc:.2f}%")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        progress_bar = tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è", unit="batch")
        with torch.no_grad():
            for inputs, labels in progress_bar:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
                acc = 100 * correct / total
                progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")
        val_loss = val_loss / len(val_loader.dataset)
        val_acc = 100 * correct / total
        logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è: –ü–æ—Ç–µ—Ä–∏: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_acc:.2f}%")

    # –≠—Ç–∞–ø 2: Fine-tuning
    logger.info("üöÄ –≠—Ç–∞–ø 2: —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤...")
    model.features[-20:].train()
    for param in model.features[:-20].parameters():
        param.requires_grad = False

    optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.01)
    for epoch in range(5):
        model.train()
        running_loss = 0.0
        running_correct = 0
        total = 0
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/5 (Fine-tuning)", unit="batch")
        for inputs, labels in progress_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            running_correct += (predicted == labels).sum().item()
            total += labels.size(0)
            acc = 100 * running_correct / total
            progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = 100 * running_correct / total
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch+1}/5, –ü–æ—Ç–µ—Ä–∏: {epoch_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {epoch_acc:.2f}%")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        progress_bar = tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è (Fine-tuning)", unit="batch")
        with torch.no_grad():
            for inputs, labels in progress_bar:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
                acc = 100 * correct / total
                progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")
        val_loss = val_loss / len(val_loader.dataset)
        val_acc = 100 * correct / total
        logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è: –ü–æ—Ç–µ—Ä–∏: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_acc:.2f}%")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ PyTorch...")
    try:
        os.makedirs(MODEL_DIR, exist_ok=True)
        torch.save(model.state_dict(), MODEL_OUT)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_OUT}")
    except Exception as e:
        logger.error(f"üö® –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ PyTorch –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑ —Å–≤–∏–Ω–µ–π")
    parser.add_argument('--convert', action='store_true', help="–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å YOLO-–¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç PyTorch")
    args = parser.parse_args()

    db = DatabaseManager(db_path=DB_PATH)

    if args.convert:
        convert_yolo_to_tf(db=db)

    train_model(db)
    db.close()


if __name__ == '__main__':
    main()