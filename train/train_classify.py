#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
üèãÔ∏è‚Äç‚ôÇÔ∏è –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è YOLO-–¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ PyTorch –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑ —Å–≤–∏–Ω–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—ë–≥–∫–æ–π –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.
"""

import os
import sys
import argparse
import cv2
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms, datasets
from torch.utils.data import DataLoader
from tqdm import tqdm
import logging

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from app.db import DatabaseManager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])
logger = logging.getLogger(__name__)

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 20
VAL_SPLIT = 0.20
SEED = 42
DATASET_DIR = os.path.join(os.path.dirname(__file__), "dataset")
MODEL_DIR = os.getenv('MODEL_DIR', 'models')
MODEL_OUT = os.getenv('PT_MODEL_PATH', os.path.join(MODEL_DIR, "pig_model.pth"))
DB_PATH = os.getenv('DB_PATH', 'pig_states.db')
IMG_DIR = os.getenv('IMG_DIR', 'train/images')
LABEL_DIR = os.getenv('LABEL_DIR', 'train/labels')


def convert_yolo_to_tf(img_dir=IMG_DIR, label_dir=LABEL_DIR, out_dir=DATASET_DIR, db=None):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è YOLO-–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PyTorch (ImageFolder)."""
    logger.info("üîß –ù–∞—á–∞–ª–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ YOLO-–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PyTorch")
    class_names = db.load_class_names()

    if os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    os.makedirs(out_dir)

    for cls in class_names:
        cls_dir = os.path.join(out_dir, cls)
        os.makedirs(cls_dir, exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∞: {cls_dir}")

    processed, skipped = 0, 0

    for filename in os.listdir(img_dir):
        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')):
            continue

        name = os.path.splitext(filename)[0]
        img_path = os.path.join(img_dir, filename)
        label_path = os.path.join(label_dir, f"{name}.txt")

        if not os.path.exists(label_path):
            logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –º–µ—Ç–∫–∏: {label_path}")
            skipped += 1
            continue

        img = cv2.imread(img_path)
        if img is None:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img_path}")
            skipped += 1
            continue

        h, w = img.shape[:2]

        with open(label_path, "r") as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]

        if not lines:
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π .txt —Ñ–∞–π–ª: {label_path}")
            skipped += 1
            continue

        for i, line in enumerate(lines):
            try:
                cls_id, x, y, bw, bh = map(float, line.split())
                cls_id = int(cls_id)
                if cls_id >= len(class_names):
                    logger.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π ID –∫–ª–∞—Å—Å–∞ {cls_id} –≤ {label_path}")
                    skipped += 1
                    continue
                cls_name = class_names[cls_id]
            except Exception as e:
                logger.error(f"üö® –û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ —Å—Ç—Ä–æ–∫–∏ –≤ {label_path}: {e}")
                skipped += 1
                continue

            cx, cy, bw, bh = x * w, y * h, bw * w, bh * h
            x1, y1 = int(cx - bw / 2), int(cy - bh / 2)
            x2, y2 = int(cx + bw / 2), int(cy + bh / 2)

            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)

            roi = img[y1:y2, x1:x2]
            if roi.size == 0 or (x2 - x1) < 5 or (y2 - y1) < 5:
                logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π/—Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π bbox –≤ {label_path}")
                skipped += 1
                continue

            out_path = os.path.join(out_dir, cls_name, f"{name}_{i}.jpg")
            cv2.imwrite(out_path, roi)
            processed += 1

    logger.info(f"üéâ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")


def get_transforms(mode="light"):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è train/val"""
    if mode == "light":
        train_transform = transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.RandomAffine(degrees=0, scale=(0.9, 1.1)),
            transforms.RandomGrayscale(p=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    else:  # heavy
        train_transform = transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.RandomApply([transforms.ColorJitter(
                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05
            )], p=0.7),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.2),
            transforms.RandomRotation(degrees=25),
            transforms.RandomAffine(
                degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10
            ),
            transforms.RandomPerspective(distortion_scale=0.3, p=0.4),
            transforms.RandomGrayscale(p=0.1),
            transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),
            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

    val_transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    return train_transform, val_transform


def train_model(db, aug_mode="light"):
    """–û–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ PyTorch –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    train_transform, val_transform = get_transforms(mode=aug_mode)

    logger.info("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    full_dataset = datasets.ImageFolder(DATASET_DIR)
    train_size = int((1 - VAL_SPLIT) * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED)
    )
    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    class_names = db.load_class_names()
    num_classes = len(class_names)
    logger.info(f"üè∑Ô∏è –ö–ª–∞—Å—Å—ã: {class_names}")

    model = models.mobilenet_v3_small(weights='DEFAULT')
    model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)
    model = model.to(device)
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å MobileNetV3Small –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    logger.info("üöÄ –≠—Ç–∞–ø 1: –æ–±—É—á–µ–Ω–∏–µ –≤–µ—Ä—Ö—É—à–∫–∏...")
    for param in model.features.parameters():
        param.requires_grad = False

    for epoch in range(EPOCHS):
        model.train()
        running_loss, running_correct, total = 0.0, 0, 0
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{EPOCHS}", unit="batch")
        for inputs, labels in progress_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            running_correct += (predicted == labels).sum().item()
            total += labels.size(0)
            acc = 100 * running_correct / total
            progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")

        logger.info(f"–≠–ø–æ—Ö–∞ {epoch+1}, –ü–æ—Ç–µ—Ä–∏: {running_loss/len(train_loader.dataset):.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {100*running_correct/total:.2f}%")

    logger.info("üöÄ –≠—Ç–∞–ø 2: Fine-tuning –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤...")
    for param in model.features[-3:].parameters():  # —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ö–≤–æ—Å—Ç —Å–µ—Ç–∏
        param.requires_grad = True
    optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.01)

    for epoch in range(5):
        model.train()
        running_loss, running_correct, total = 0.0, 0, 0
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/5 (Fine-tuning)", unit="batch")
        for inputs, labels in progress_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            running_correct += (predicted == labels).sum().item()
            total += labels.size(0)
            acc = 100 * running_correct / total
            progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")

        logger.info(f"Fine-tuning {epoch+1}, –ü–æ—Ç–µ—Ä–∏: {running_loss/len(train_loader.dataset):.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {100*running_correct/total:.2f}%")

    os.makedirs(MODEL_DIR, exist_ok=True)
    torch.save(model.state_dict(), MODEL_OUT)
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_OUT}")


def main():
    parser = argparse.ArgumentParser(description="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ PyTorch –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑ —Å–≤–∏–Ω–µ–π")
    parser.add_argument('--convert', action='store_true', help="–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å YOLO-–¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç PyTorch")
    parser.add_argument('--light-aug', action='store_true', help="–õ—ë–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
    parser.add_argument('--heavy-aug', action='store_true', help="–°–∏–ª—å–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
    args = parser.parse_args()

    db = DatabaseManager(db_path=DB_PATH)

    if args.convert:
        convert_yolo_to_tf(db=db)

    if args.heavy_aug:
        aug_mode = "heavy"
    else:
        aug_mode = "light"

    train_model(db, aug_mode=aug_mode)
    db.close()


if __name__ == '__main__':
    main()
