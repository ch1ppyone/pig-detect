#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
üèãÔ∏è‚Äç‚ôÇÔ∏è –û–±—É—á–µ–Ω–∏–µ MobileNetV3Small –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ–∑ —Å–≤–∏–Ω–µ–π (ImageFolder).
–î–æ–±–∞–≤–ª–µ–Ω EarlyStopping –∏ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è.
"""

import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms, datasets
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm

# –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 20
VAL_SPLIT = 0.2
SEED = 42
MODEL_OUT = "pig_model_best.pth"


class EarlyStopping:
    def __init__(self, patience=5):
        self.patience = patience
        self.counter = 0
        self.best_acc = 0
        self.early_stop = False

    def step(self, val_acc):
        if val_acc > self.best_acc:
            self.best_acc = val_acc
            self.counter = 0
            return True  # –Ω–æ–≤–æ–µ –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
            return False


def get_transforms(mode="light", image_size=IMAGE_SIZE):
    if mode == "light":
        train_transform = transforms.Compose([
            transforms.Resize(image_size),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5],
                                 std=[0.5, 0.5, 0.5])
        ])
    else:  # heavy
        train_transform = transforms.Compose([
            transforms.Resize(image_size),
            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.3),
            transforms.RandomRotation(30),
            transforms.RandomPerspective(distortion_scale=0.4, p=0.5),
            transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.7, 1.3), shear=15),
            transforms.RandomGrayscale(p=0.2),
            transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.4),
            transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),
            transforms.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5],
                                 std=[0.5, 0.5, 0.5])
        ])
    val_transform = transforms.Compose([
        transforms.Resize(image_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
    ])
    return train_transform, val_transform


def train_model(dataset_dir, aug_mode, device, epochs, batch_size, image_size, out_path):
    device = torch.device(device if torch.cuda.is_available() and device == "cuda" else "cpu")
    print(f"üîß –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    train_transform, val_transform = get_transforms(mode=aug_mode, image_size=image_size)

    print("üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...")
    full_dataset = datasets.ImageFolder(dataset_dir)
    num_classes = len(full_dataset.classes)
    print(f"üè∑Ô∏è –ö–ª–∞—Å—Å—ã: {full_dataset.classes}")

    train_size = int((1 - VAL_SPLIT) * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(SEED)
    )
    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    model = models.mobilenet_v3_small(weights="DEFAULT")
    model.classifier[3] = nn.Sequential(
        nn.Linear(model.classifier[3].in_features, 256),
        nn.ReLU(),
        nn.Dropout(p=0.4),
        nn.Linear(256, num_classes)
    )
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)

    early_stopping = EarlyStopping(patience=5)

    best_path = out_path
    best_acc = 0

    for epoch in range(epochs):
        model.train()
        running_loss, running_correct, total = 0.0, 0, 0
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}", unit="batch")
        for inputs, labels in progress_bar:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            running_correct += (preds == labels).sum().item()
            total += labels.size(0)

            acc = 100 * running_correct / total
            progress_bar.set_postfix(loss=loss.item(), acc=f"{acc:.2f}%")

        train_loss = running_loss / len(train_loader.dataset)
        train_acc = 100 * running_correct / total
        print(f"üìä Train {epoch+1}: loss {train_loss:.4f}, acc {train_acc:.2f}%")

        # –≤–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)

        val_loss /= len(val_loader.dataset)
        val_acc = 100 * val_correct / val_total
        print(f"üß™ Val {epoch+1}: loss {val_loss:.4f}, acc {val_acc:.2f}%")

        # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if early_stopping.step(val_acc):
            torch.save(model.state_dict(), best_path)
            print(f"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (val_acc={val_acc:.2f}%)")

        if early_stopping.early_stop:
            print("‚èπÔ∏è Early stopping: —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è")
            break

    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {early_stopping.best_acc:.2f}%")


def main():
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ MobileNetV3Small –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑ —Å–≤–∏–Ω–µ–π")
    parser.add_argument("--dataset", type=str, default="dataset", help="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É (ImageFolder)")
    parser.add_argument("--epochs", type=int, default=EPOCHS, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--device", choices=["cpu", "cuda"], default="cuda", help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")
    parser.add_argument("--imgsz", type=int, default=224, help="–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    parser.add_argument("--heavy-aug", action="store_true", help="–°–∏–ª—å–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
    parser.add_argument("--out", type=str, default=MODEL_OUT, help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    args = parser.parse_args()

    aug_mode = "heavy" if args.heavy_aug else "light"
    train_model(
        dataset_dir=args.dataset,
        aug_mode=aug_mode,
        device=args.device,
        epochs=args.epochs,
        batch_size=args.batch_size,
        image_size=(args.imgsz, args.imgsz),
        out_path=args.out
    )


if __name__ == "__main__":
    main()
